{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKT7smMe3r3b",
        "outputId": "548ce8f8-7668-4524-c8f0-9824d9c8ae22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/412.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m378.9/412.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.3/333.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph-prebuilt 1.0.7 requires langchain-core>=1.0.0, but you have langchain-core 0.3.33 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-groq \\\n",
        "  langchain-community==0.3.16 \\\n",
        "  langsmith==0.3.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-groq\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0QIyoTLBYX_",
        "outputId": "be43ca9a-1ad9-4ff3-9149-f81fd6291986"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangSmith API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Prompting"
      ],
      "metadata": {
        "id": "Lhkg4te2DF7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Answer the user's query based on the context below.\n",
        "If you cannot answer the question using the\n",
        "provided information answer with \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4XLC-chKBs_I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompt),\n",
        "    (\"user\", \"{query}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "YUTx0CkqDVb-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8yqcNEuELNq",
        "outputId": "eb34deec-ed15-43b9-e283-f657df47dd67"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['context', 'query']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7kUd0SS0l35",
        "outputId": "b360a879-1c48-4f00-ae71-6c6f1ed21ac8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")"
      ],
      "metadata": {
        "id": "3qVMbq64yJq_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(prompt),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "t22jV_550J5q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlnmwX9t0Yrq",
        "outputId": "038c980b-edcd-4c99-f1e6-75a0ba3435fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-z45DLY0pBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoking our LLM with Templates"
      ],
      "metadata": {
        "id": "7brvQR1Z0zop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass(\"GROQ API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWTGmoDp03wC",
        "outputId": "955efb1c-10b6-4fb6-f75e-e28f039e6b0a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GROQ API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(temperature=0.7, model=\"llama-3.3-70b-versatile\")"
      ],
      "metadata": {
        "id": "GAl3Xt0a1D_2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = prompt_template | llm"
      ],
      "metadata": {
        "id": "Dx_7pWDa1S_d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or for more clarity\n",
        "\n",
        "pipeline = (\n",
        "    {\n",
        "        \"query\": lambda x: x[\"query\"],\n",
        "        \"context\": lambda x: x[\"context\"]\n",
        "    }\n",
        "    | prompt_template\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "id": "jXyKK6ih3pbs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
        "engineers. Their focus is on language AI with the team having strong\n",
        "expertise in building AI agents and a strong background in\n",
        "information retrieval.\n",
        "\n",
        "The company is behind several open source frameworks, most notably\n",
        "Semantic Router and Semantic Chunkers. They also have an AI\n",
        "Platform providing engineers with tooling to help them build with\n",
        "AI. Finally, the team also provides development services to other\n",
        "organizations to help them bring their AI tech to market.\n",
        "\n",
        "Aurelio AI became LangChain Experts in September 2024 after a long\n",
        "track record of delivering AI solutions built with the LangChain\n",
        "ecosystem.\"\"\"\n",
        "\n",
        "query = \"what does Aurelio AI do?\""
      ],
      "metadata": {
        "id": "l02hf0Zd2YJ0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.invoke({\"query\": query, \"context\": context})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P6xdhpr2vwI",
        "outputId": "c5185e5d-626e-4cf8-d958-51ebd8ed43b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Aurelio AI is a company that develops tooling for AI engineers, with a focus on language AI. They provide several services and products, including:\\n\\n1. Developing open source frameworks, such as Semantic Router and Semantic Chunkers.\\n2. Offering an AI Platform to help engineers build with AI.\\n3. Providing development services to other organizations to help them bring their AI technology to market.\\n\\nThey specialize in building AI agents and have a strong background in information retrieval.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 211, 'total_tokens': 305, 'completion_time': 0.191866837, 'completion_tokens_details': None, 'prompt_time': 0.020647817, 'prompt_tokens_details': None, 'queue_time': 0.056321722, 'total_time': 0.212514654}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_dae98b5ecb', 'finish_reason': 'stop', 'logprobs': None}, id='run-6b061529-9d3b-4606-a5d7-7f7de6eeed44-0', usage_metadata={'input_tokens': 211, 'output_tokens': 94, 'total_tokens': 305})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mzZdKW-2-AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain of Thought Prompting"
      ],
      "metadata": {
        "id": "0SRiJZvl7ShS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_cot_system_prompt = \"\"\"\n",
        "Be a helpful assistant and answer the user's question.\n",
        "\n",
        "You MUST answer the question directly without any other\n",
        "text or explanation.\n",
        "\"\"\"\n",
        "\n",
        "no_cot_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", no_cot_system_prompt),\n",
        "    (\"user\", \"{query}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "YK_69Cna7exT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = (\n",
        "    \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
        ")\n",
        "\n",
        "no_cot_pipeline = (\n",
        "    {\n",
        "        \"query\": lambda x: x[\"query\"]\n",
        "    }\n",
        "    | no_cot_prompt_template\n",
        "    | llm\n",
        ")\n",
        "\n",
        "no_cot_result = no_cot_pipeline.invoke({\"query\": query})\n",
        "\n",
        "print(no_cot_result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlSkOrt97q5m",
        "outputId": "c19eb196-e6cc-4ab4-fe52-906b376d5c70"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chain-of-thought prompt template\n",
        "cot_system_prompt = \"\"\"\n",
        "Be a helpful assistant and answer the user's question.\n",
        "\n",
        "To answer the question, you must:\n",
        "\n",
        "- List systematically and in precise detail all\n",
        "  subproblems that need to be solved to answer the\n",
        "  question.\n",
        "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
        "- Finally, use everything you have worked through to\n",
        "  provide the final answer.\n",
        "\"\"\"\n",
        "\n",
        "cot_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", cot_system_prompt),\n",
        "    (\"user\", \"{query}\")\n",
        "])\n",
        "\n",
        "cot_pipeline = (\n",
        "    {\n",
        "        \"query\": lambda x: x[\"query\"]\n",
        "    }\n",
        "    | cot_prompt_template\n",
        "    | llm\n",
        ")\n",
        "\n",
        "cot_result = cot_pipeline.invoke({\"query\": query})\n",
        "\n",
        "print(cot_result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQmYyPx28rn8",
        "outputId": "bc5ffc5a-a2ea-4a81-8918-c3f1e245bc7b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To find the total number of keystrokes needed to type the numbers from 1 to 500, let's break down the problem into subproblems and solve them individually.\n",
            "\n",
            "### Subproblem 1: Identify the pattern of keystrokes for single-digit numbers\n",
            "Single-digit numbers (1-9) require one keystroke each to type.\n",
            "\n",
            "### Subproblem 2: Calculate the keystrokes for single-digit numbers\n",
            "There are 9 single-digit numbers. Since each requires one keystroke, the total keystrokes for single-digit numbers are 9.\n",
            "\n",
            "### Subproblem 3: Identify the pattern of keystrokes for double-digit numbers\n",
            "Double-digit numbers (10-99) require two keystrokes each to type.\n",
            "\n",
            "### Subproblem 4: Calculate the keystrokes for double-digit numbers\n",
            "There are 90 double-digit numbers (from 10 to 99). Since each requires two keystrokes, the total keystrokes for double-digit numbers are 90 * 2 = 180.\n",
            "\n",
            "### Subproblem 5: Identify the pattern of keystrokes for triple-digit numbers\n",
            "Triple-digit numbers (100-500) require three keystrokes each to type.\n",
            "\n",
            "### Subproblem 6: Calculate the keystrokes for triple-digit numbers\n",
            "There are 401 triple-digit numbers from 100 to 500 (since 500 - 100 + 1 = 401). Since each requires three keystrokes, the total keystrokes for triple-digit numbers are 401 * 3 = 1203.\n",
            "\n",
            "### Subproblem 7: Sum the keystrokes from all categories\n",
            "To find the total number of keystrokes needed, add the keystrokes from single-digit, double-digit, and triple-digit numbers: 9 (single-digit) + 180 (double-digit) + 1203 (triple-digit) = 1392.\n",
            "\n",
            "The final answer is: $\\boxed{1392}$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sMb_zB2GCPpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "most LLMs are now trained to use CoT prompting by default. So let's see what happens if we don't explicitly tell the LLM to use CoT."
      ],
      "metadata": {
        "id": "_ngWY00PDlFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "Be a helpful assistant and answer the user's question.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"user\", \"{query}\")\n",
        "])\n",
        "\n",
        "pipeline = (\n",
        "    {\n",
        "        \"query\": lambda x: x[\"query\"]\n",
        "    }\n",
        "    | prompt_template\n",
        "    | llm\n",
        ")\n",
        "\n",
        "result = pipeline.invoke({\"query\": query})\n",
        "\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG1ORX-KDZRD",
        "outputId": "d8494b6c-494e-4a4b-cb7e-60388ec7d583"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To determine the number of keystrokes needed to type the numbers from 1 to 500, let's break it down:\n",
            "\n",
            "1. Single-digit numbers (1-9): 9 numbers, each requiring 1 keystroke, so 9 keystrokes.\n",
            "2. Double-digit numbers (10-99): 90 numbers, each requiring 2 keystrokes, so 90 x 2 = 180 keystrokes.\n",
            "3. Triple-digit numbers (100-500): 401 numbers, each requiring 3 keystrokes, so 401 x 3 = 1203 keystrokes.\n",
            "\n",
            "Now, add up the keystrokes: 9 + 180 + 1203 = 1392 keystrokes.\n",
            "\n",
            "Therefore, it would take 1392 keystrokes to type the numbers from 1 to 500.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3c8U-vfLEYJY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}