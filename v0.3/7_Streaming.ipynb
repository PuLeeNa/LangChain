{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRAjle-jz-ML",
        "outputId": "e2c51c34-8f75-4c0f-8080-acaaa2a5be20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/412.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m389.1/412.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph-prebuilt 1.0.7 requires langchain-core>=1.0.0, but you have langchain-core 0.3.33 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-groq \\\n",
        "  langchain-community==0.3.16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\") or \\\n",
        "    getpass(\"Enter your GROQ API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hEIYyEU_c_O",
        "outputId": "1210a625-aa6c-4638-929b-22f2508de890"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(temperature=0.0, model=\"llama-3.3-70b-versatile\", streaming=True)"
      ],
      "metadata": {
        "id": "rx3nxeZ2_pZ6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_out = llm.invoke(\"Hello there\")\n",
        "llm_out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qMzipMZ_06L",
        "outputId": "f0c648c0-736f-473c-e71c-38611120cf8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run-f2fa0006-8c65-4aad-a620-86a7a65af39e-0', usage_metadata={'input_tokens': 37, 'output_tokens': 25, 'total_tokens': 62})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming with astream\n"
      ],
      "metadata": {
        "id": "BqO6gv4vAFsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "async for token in llm.astream(\"What is NLP?\"):\n",
        "  tokens.append(token)\n",
        "  print(token.content, end=\"!\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5yVS2Ma_7DK",
        "outputId": "460b9446-a18a-4c64-c87c-c56995ad74e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!N!LP! stands! for! Natural! Language! Processing!,! which! is! a! sub!field! of! artificial! intelligence! (!AI!)! that! deals! with! the! interaction! between! computers! and! humans! in! natural! language!.! It! is! a! multid!isc!iplinary! field! that! combines! computer! science!,! lingu!istics!,! and! cognitive! psychology! to! enable! computers! to! process!,! understand!,! and! generate! human! language!.\n",
            "\n",
            "!N!LP! involves! a! range! of! techniques! and! technologies!,! including!:\n",
            "\n",
            "!1!.! **!Text! analysis!**:! extracting! meaning! and! insights! from! text! data!,! such! as! sentiment! analysis!,! entity! recognition!,! and! topic! modeling!.\n",
            "!2!.! **!Language! understanding!**:! enabling! computers! to! comprehend! the! meaning! of! text!,! including! syntax!,! semantics!,! and! prag!m!atics!.\n",
            "!3!.! **!Language! generation!**:! generating! human!-like! text!,! such! as! chat!bots!,! language! translation!,! and! text! summar!ization!.\n",
            "!4!.! **!Speech! recognition!**:! converting! spoken! language! into! text!,! such! as! voice! assistants! and! speech!-to!-text! systems!.\n",
            "\n",
            "!N!LP! has! many! applications!,! including!:\n",
            "\n",
            "!1!.! **!Virtual! assistants!**:! like! Siri!,! Alexa!,! and! Google! Assistant!,! which! use! N!LP! to! understand! voice! commands! and! respond! accordingly!.\n",
            "!2!.! **!Language! translation!**:! such! as! Google! Translate!,! which! uses! N!LP! to! translate! text! and! speech! from! one! language! to! another!.\n",
            "!3!.! **!Sent!iment! analysis!**:! analyzing! text! data! to! determine! the! sentiment! or! emotional! tone! of! the! text!,! such! as! in! customer! feedback! or! social! media! monitoring!.\n",
            "!4!.! **!Chat!bots!**:! using! N!LP! to! generate! human!-like! responses! to! customer! inquiries! or! support! requests!.\n",
            "!5!.! **!Text! summar!ization!**:! summar!izing! long! documents! or! articles! into! shorter!,! more! digest!ible! versions!.\n",
            "\n",
            "!Some! of! the! key! techniques! used! in! N!LP! include!:\n",
            "\n",
            "!1!.! **!Machine! learning!**:! using! machine! learning! algorithms! to! train! models! on! large! datasets! and! improve! their! performance! over! time!.\n",
            "!2!.! **!Deep! learning!**:! using! deep! neural! networks! to! analyze! and! generate! text! data!.\n",
            "!3!.! **!Rule!-based! systems!**:! using! predefined! rules! and! grammar! to! analyze! and! generate! text! data!.\n",
            "!4!.! **!Hy!brid! approaches!**:! combining! multiple! techniques!,! such! as! machine! learning! and! rule!-based! systems!,! to! achieve! better! results!.\n",
            "\n",
            "!Overall!,! N!LP! is! a! rapidly! evolving! field! that! has! many! exciting! applications! and! opportunities! for! innovation! and! research!.!!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv7lMxgNAuQq",
        "outputId": "94f0ce52-3377-46d6-ac60-f2bacf0adfdc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-20bc6d37-ea1b-4cc9-8dbb-6f389fa7bc07')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp2h-5GrBP7r",
        "outputId": "e4426c6d-9930-48b6-b488-c0b8f678aaa8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessageChunk(content='N', additional_kwargs={}, response_metadata={}, id='run-20bc6d37-ea1b-4cc9-8dbb-6f389fa7bc07')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[0] + tokens[1] + tokens[2] + tokens[3] + tokens[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-0g6XCBBS-S",
        "outputId": "d0ad0215-6ee5-45e7-cf5f-82e44af2a19c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessageChunk(content='NLP stands for', additional_kwargs={}, response_metadata={}, id='run-20bc6d37-ea1b-4cc9-8dbb-6f389fa7bc07')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming with Agents\n",
        "\n",
        "To construct the agent executor we need:\n",
        "\n",
        "1. Tools\n",
        "2. ChatPromptTemplate\n",
        "3. Our LLM (already defined with llm)\n",
        "4. An agent\n",
        "5. Finally, the agent executor"
      ],
      "metadata": {
        "id": "tBuVdzT6BfNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "Qztg8P-_Bysg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def add(x: float, y: float) -> float:\n",
        "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "@tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
        "    return x * y\n",
        "\n",
        "@tool\n",
        "def exponentiate(x: float, y: float) -> float:\n",
        "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
        "    return x ** y\n",
        "\n",
        "@tool\n",
        "def subtract(x: float, y: float) -> float:\n",
        "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
        "    return y - x\n",
        "\n",
        "@tool\n",
        "def final_answer(answer: str, tools_used: list[str]) -> str:\n",
        "    \"\"\"Use this tool to provide a final answer to the user.\n",
        "    The answer should be in natural language as this will be provided\n",
        "    to the user directly. The tools_used must include a list of tool\n",
        "    names that were used within the `scratchpad`. You MUST use this tool\n",
        "    to conclude the interaction.\n",
        "    \"\"\"\n",
        "    return {\"answer\": answer, \"tools_used\": tools_used}"
      ],
      "metadata": {
        "id": "Qqfh2qKBBYDC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [add, multiply, exponentiate, subtract, final_answer]"
      ],
      "metadata": {
        "id": "IjqoqAUHCOxe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatPromptTemplate"
      ],
      "metadata": {
        "id": "GOAWOLT-CbN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You're a helpful assistant. When answering a user's question \"\n",
        "        \"you should first use one of the tools provided. After using a \"\n",
        "        \"tool the tool output will be provided back to you. You MUST \"\n",
        "        \"then use the final_answer tool to provide a final answer to the user. \"\n",
        "        \"DO NOT use the same tool more than once.\"\n",
        "    )),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])"
      ],
      "metadata": {
        "id": "_ZVshiJuCaYL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "JHRLPoXNClVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.base import RunnableSerializable\n",
        "\n",
        "tools = [add, subtract, multiply, exponentiate, final_answer]\n",
        "\n",
        "# define the agent runnable\n",
        "agent: RunnableSerializable = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "    }\n",
        "    | prompt\n",
        "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
        ")"
      ],
      "metadata": {
        "id": "vI90yPinChSt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Executor"
      ],
      "metadata": {
        "id": "udSdh01JDJgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "\n",
        "\n",
        "# create tool name to function mapping\n",
        "name2tool = {tool.name: tool.func for tool in tools}\n",
        "\n",
        "class CustomAgentExecutor:\n",
        "    chat_history: list[BaseMessage]\n",
        "\n",
        "    def __init__(self, max_iterations: int = 3):\n",
        "        self.chat_history = []\n",
        "        self.max_iterations = max_iterations\n",
        "        self.agent: RunnableSerializable = (\n",
        "            {\n",
        "                \"input\": lambda x: x[\"input\"],\n",
        "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "            }\n",
        "            | prompt\n",
        "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
        "        )\n",
        "\n",
        "    def invoke(self, input: str) -> dict:\n",
        "        # invoke the agent but we do this iteratively in a loop until\n",
        "        # reaching a final answer\n",
        "        count = 0\n",
        "        agent_scratchpad = []\n",
        "        while count < self.max_iterations:\n",
        "            # invoke a step for the agent to generate a tool call\n",
        "            out = self.agent.invoke({\n",
        "                \"input\": input,\n",
        "                \"chat_history\": self.chat_history,\n",
        "                \"agent_scratchpad\": agent_scratchpad\n",
        "            })\n",
        "            # if the tool call is the final answer tool, we stop\n",
        "            if out.tool_calls[0][\"name\"] == \"final_answer\":\n",
        "                break\n",
        "            agent_scratchpad.append(out)  # add tool call to scratchpad\n",
        "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
        "            tool_out = name2tool[out.tool_calls[0][\"name\"]](**out.tool_calls[0][\"args\"])\n",
        "            # add the tool output to the agent scratchpad\n",
        "            action_str = f\"The {out.tool_calls[0]['name']} tool returned {tool_out}\"\n",
        "            agent_scratchpad.append({\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": action_str,\n",
        "                \"tool_call_id\": out.tool_calls[0][\"id\"]\n",
        "            })\n",
        "            # add a print so we can see intermediate steps\n",
        "            print(f\"{count}: {action_str}\")\n",
        "            count += 1\n",
        "        # add the final output to the chat history\n",
        "        final_answer = out.tool_calls[0][\"args\"]\n",
        "        # this is a dictionary, so we convert it to a string for compatibility with\n",
        "        # the chat history\n",
        "        final_answer_str = json.dumps(final_answer)\n",
        "        self.chat_history.append({\"input\": input, \"output\": final_answer_str})\n",
        "        self.chat_history.extend([\n",
        "            HumanMessage(content=input),\n",
        "            AIMessage(content=final_answer_str)\n",
        "        ])\n",
        "        # return the final answer in dict form\n",
        "        return final_answer\n",
        "\n",
        "agent_executor = CustomAgentExecutor()"
      ],
      "metadata": {
        "id": "CLw6ASuCC1FF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke(input=\"What is 10 + 10\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-8dqkMJDO0A",
        "outputId": "010b509a-ed9f-4774-81a9-e2510258344e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: The add tool returned 20\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'The answer to 10 + 10 is 20.', 'tools_used': ['add']}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's modify our agent_executor to use streaming and parse the streamed output into a format that we can more easily work with."
      ],
      "metadata": {
        "id": "zSWdXUNOEHQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model_name=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.0,\n",
        "    streaming=True\n",
        ").configurable_fields(\n",
        "    callbacks=ConfigurableField(\n",
        "        id=\"callbacks\",\n",
        "        name=\"callbacks\",\n",
        "        description=\"A list of callbacks to use for streaming\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "WeihdpTyDvNX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We reinitialize our agent, nothing changes here:"
      ],
      "metadata": {
        "id": "KmHATspyD9ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent: RunnableSerializable = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "    }\n",
        "    | prompt\n",
        "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
        ")"
      ],
      "metadata": {
        "id": "xZBsLt5IDvsg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from langchain.callbacks.base import AsyncCallbackHandler\n",
        "\n",
        "\n",
        "class QueueCallbackHandler(AsyncCallbackHandler):\n",
        "    \"\"\"Callback handler that puts tokens into a queue.\"\"\"\n",
        "\n",
        "    def __init__(self, queue: asyncio.Queue):\n",
        "        self.queue = queue\n",
        "        self.final_answer_seen = False\n",
        "\n",
        "    async def __aiter__(self):\n",
        "        while True:\n",
        "            if self.queue.empty():\n",
        "                await asyncio.sleep(0.1)\n",
        "                continue\n",
        "            token_or_done = await self.queue.get()\n",
        "\n",
        "            if token_or_done == \"<<DONE>>\":\n",
        "                # this means we're done\n",
        "                return\n",
        "            if token_or_done:\n",
        "                yield token_or_done\n",
        "\n",
        "    async def on_llm_new_token(self, *args, **kwargs) -> None:\n",
        "        \"\"\"Put new token in the queue.\"\"\"\n",
        "        #print(f\"on_llm_new_token: {args}, {kwargs}\")\n",
        "        chunk = kwargs.get(\"chunk\")\n",
        "        if chunk:\n",
        "            # check for final_answer tool call\n",
        "            if tool_calls := chunk.message.additional_kwargs.get(\"tool_calls\"):\n",
        "                if tool_calls[0][\"function\"][\"name\"] == \"final_answer\":\n",
        "                    # this will allow the stream to end on the next `on_llm_end` call\n",
        "                    self.final_answer_seen = True\n",
        "        await self.queue.put(chunk)\n",
        "        return\n",
        "\n",
        "    async def on_llm_end(self, *args, **kwargs) -> None:\n",
        "        \"\"\"Put None in the queue to signal completion.\"\"\"\n",
        "        #print(f\"on_llm_end: {args}, {kwargs}\")\n",
        "        # this should only be used at the end of our agent execution, however LangChain\n",
        "        # will call this at the end of every tool call, not just the final tool call\n",
        "        # so we must only send the \"done\" signal if we have already seen the final_answer\n",
        "        # tool call\n",
        "        if self.final_answer_seen:\n",
        "            await self.queue.put(\"<<DONE>>\")\n",
        "        else:\n",
        "            await self.queue.put(\"<<STEP_END>>\")\n",
        "        return"
      ],
      "metadata": {
        "id": "jX416st2EO7A"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "tokens = []\n",
        "\n",
        "async def stream(query: str):\n",
        "    response = agent.with_config(\n",
        "        callbacks=[streamer]\n",
        "    )\n",
        "    async for token in response.astream({\n",
        "        \"input\": query,\n",
        "        \"chat_history\": [],\n",
        "        \"agent_scratchpad\": []\n",
        "    }):\n",
        "        tokens.append(token)\n",
        "        print(token, flush=True)\n",
        "\n",
        "await stream(\"What is 10 + 10\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U69abLlq96Vf",
        "outputId": "56ed2ca8-7483-4c07-88a0-91e56340b640"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='' additional_kwargs={} response_metadata={} id='run-460425d2-e510-4b1c-a006-66f96aa41e5a'\n",
            "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': '4ech362jv', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}]} response_metadata={} id='run-460425d2-e510-4b1c-a006-66f96aa41e5a' tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': '4ech362jv', 'type': 'tool_call'}] tool_call_chunks=[{'name': 'add', 'args': '{\"x\":10,\"y\":10}', 'id': '4ech362jv', 'index': 0, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={} response_metadata={'finish_reason': 'tool_calls'} id='run-460425d2-e510-4b1c-a006-66f96aa41e5a' usage_metadata={'input_tokens': 687, 'output_tokens': 16, 'total_tokens': 703}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tk = tokens[0]\n",
        "\n",
        "for token in tokens[1:]:\n",
        "    tk += token\n",
        "\n",
        "tk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoxcODeP99At",
        "outputId": "b843aba0-d7a3-4be6-ab58-f299913bf0ff"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': '4ech362jv', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-460425d2-e510-4b1c-a006-66f96aa41e5a', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': '4ech362jv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 687, 'output_tokens': 16, 'total_tokens': 703}, tool_call_chunks=[{'name': 'add', 'args': '{\"x\":10,\"y\":10}', 'id': '4ech362jv', 'index': 0, 'type': 'tool_call_chunk'}])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "class CustomAgentExecutor:\n",
        "    chat_history: list[BaseMessage]\n",
        "\n",
        "    def __init__(self, max_iterations: int = 3):\n",
        "        self.chat_history = []\n",
        "        self.max_iterations = max_iterations\n",
        "        self.agent: RunnableSerializable = (\n",
        "            {\n",
        "                \"input\": lambda x: x[\"input\"],\n",
        "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "            }\n",
        "            | prompt\n",
        "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
        "        )\n",
        "\n",
        "    async def invoke(self, input: str, streamer: QueueCallbackHandler, verbose: bool = False) -> dict:\n",
        "        # invoke the agent but we do this iteratively in a loop until\n",
        "        # reaching a final answer\n",
        "        count = 0\n",
        "        agent_scratchpad = []\n",
        "        while count < self.max_iterations:\n",
        "            # invoke a step for the agent to generate a tool call\n",
        "            async def stream(query: str):\n",
        "                response = self.agent.with_config(\n",
        "                    callbacks=[streamer]\n",
        "                )\n",
        "                # we initialize the output dictionary that we will be populating with\n",
        "                # our streamed output\n",
        "                output = None\n",
        "                # now we begin streaming\n",
        "                async for token in response.astream({\n",
        "                    \"input\": query,\n",
        "                    \"chat_history\": self.chat_history,\n",
        "                    \"agent_scratchpad\": agent_scratchpad\n",
        "                }):\n",
        "                    if output is None:\n",
        "                        output = token\n",
        "                    else:\n",
        "                        # we can just add the tokens together as they are streamed and\n",
        "                        # we'll have the full response object at the end\n",
        "                        output += token\n",
        "                    if token.content != \"\":\n",
        "                        # we can capture various parts of the response object\n",
        "                        if verbose: print(f\"content: {token.content}\", flush=True)\n",
        "                    tool_calls = token.additional_kwargs.get(\"tool_calls\")\n",
        "                    if tool_calls:\n",
        "                        if verbose: print(f\"tool_calls: {tool_calls}\", flush=True)\n",
        "                        tool_name = tool_calls[0][\"function\"][\"name\"]\n",
        "                        if tool_name:\n",
        "                            if verbose: print(f\"tool_name: {tool_name}\", flush=True)\n",
        "                        arg = tool_calls[0][\"function\"][\"arguments\"]\n",
        "                        if arg != \"\":\n",
        "                            if verbose: print(f\"arg: {arg}\", flush=True)\n",
        "                return AIMessage(\n",
        "                    content=output.content,\n",
        "                    tool_calls=output.tool_calls,\n",
        "                    tool_call_id=output.tool_calls[0][\"id\"]\n",
        "                )\n",
        "\n",
        "            tool_call = await stream(query=input)\n",
        "            # add initial tool call to scratchpad\n",
        "            agent_scratchpad.append(tool_call)\n",
        "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
        "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
        "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
        "            tool_call_id = tool_call.tool_call_id\n",
        "            tool_out = name2tool[tool_name](**tool_args)\n",
        "            # add the tool output to the agent scratchpad\n",
        "            tool_exec = ToolMessage(\n",
        "                content=f\"{tool_out}\",\n",
        "                tool_call_id=tool_call_id\n",
        "            )\n",
        "            agent_scratchpad.append(tool_exec)\n",
        "            count += 1\n",
        "            # if the tool call is the final answer tool, we stop\n",
        "            if tool_name == \"final_answer\":\n",
        "                break\n",
        "        # add the final output to the chat history, we only add the \"answer\" field\n",
        "        final_answer = tool_out[\"answer\"]\n",
        "        self.chat_history.extend([\n",
        "            HumanMessage(content=input),\n",
        "            AIMessage(content=final_answer)\n",
        "        ])\n",
        "        # return the final answer in dict form\n",
        "        return tool_args\n",
        "\n",
        "agent_executor = CustomAgentExecutor()"
      ],
      "metadata": {
        "id": "5BgF4RS0-F6t"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "out = await agent_executor.invoke(\"What is 10 + 10\", streamer, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6s5clm3_ClM",
        "outputId": "20f624cf-ad37-4b9a-e761-35362d57f745"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tool_calls: [{'index': 0, 'id': 'tbe6bv465', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}]\n",
            "tool_name: add\n",
            "arg: {\"x\":10,\"y\":10}\n",
            "tool_calls: [{'index': 0, 'id': 'paj6wnamg', 'function': {'arguments': '{\"answer\":\"The answer to 10 + 10 is 20.\",\"tools_used\":[\"add\"]}', 'name': 'final_answer'}, 'type': 'function'}]\n",
            "tool_name: final_answer\n",
            "arg: {\"answer\":\"The answer to 10 + 10 is 20.\",\"tools_used\":[\"add\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "out = await agent_executor.invoke(\"What is 10 + 10\", streamer)"
      ],
      "metadata": {
        "id": "puIiPaZp_SoV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "task = asyncio.create_task(agent_executor.invoke(\"What is 10 + 10\", streamer))\n",
        "\n",
        "async for token in streamer:\n",
        "    print(token, flush=True)\n",
        "\n",
        "await task"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doAr66cP_mz4",
        "outputId": "ddea4065-f46d-4421-de9d-d89bad663fa7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-e1f4dabf-bf76-4b15-95f9-8b32ad18e724')\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': '95c07t9np', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={}, id='run-e1f4dabf-bf76-4b15-95f9-8b32ad18e724', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': '95c07t9np', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'add', 'args': '{\"x\":10,\"y\":10}', 'id': '95c07t9np', 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "generation_info={'finish_reason': 'tool_calls'} message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls'}, id='run-e1f4dabf-bf76-4b15-95f9-8b32ad18e724', usage_metadata={'input_tokens': 744, 'output_tokens': 16, 'total_tokens': 760})\n",
            "<<STEP_END>>\n",
            "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-50c6cce3-3c4d-4a85-be11-1faf95485744')\n",
            "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 't8a58tasz', 'function': {'arguments': '{\"answer\":\"The answer to 10 + 10 is 20\",\"tools_used\":[\"add\"]}', 'name': 'final_answer'}, 'type': 'function'}]}, response_metadata={}, id='run-50c6cce3-3c4d-4a85-be11-1faf95485744', tool_calls=[{'name': 'final_answer', 'args': {'answer': 'The answer to 10 + 10 is 20', 'tools_used': ['add']}, 'id': 't8a58tasz', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'final_answer', 'args': '{\"answer\":\"The answer to 10 + 10 is 20\",\"tools_used\":[\"add\"]}', 'id': 't8a58tasz', 'index': 0, 'type': 'tool_call_chunk'}])\n",
            "generation_info={'finish_reason': 'tool_calls'} message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls'}, id='run-50c6cce3-3c4d-4a85-be11-1faf95485744', usage_metadata={'input_tokens': 773, 'output_tokens': 29, 'total_tokens': 802})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'The answer to 10 + 10 is 20', 'tools_used': ['add']}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "task = asyncio.create_task(agent_executor.invoke(\"What is 10 + 10\", streamer))\n",
        "\n",
        "async for token in streamer:\n",
        "    # first identify if we have a <<STEP_END>> token\n",
        "    if token == \"<<STEP_END>>\":\n",
        "        print(\"\\n\", flush=True)\n",
        "    # we'll first identify if the token is a tool call\n",
        "    elif tool_calls := token.message.additional_kwargs.get(\"tool_calls\"):\n",
        "        # if we have a tool call with a tool name, we'll print it\n",
        "        if tool_name := tool_calls[0][\"function\"][\"name\"]:\n",
        "            print(f\"Calling {tool_name}...\", flush=True)\n",
        "        # if we have a tool call with arguments, we ad them to our args string\n",
        "        if tool_args := tool_calls[0][\"function\"][\"arguments\"]:\n",
        "            print(f\"{tool_args}\", end=\"\", flush=True)\n",
        "\n",
        "_ = await task"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2ntdxhbAfCZ",
        "outputId": "71e160ce-3a34-49dd-dbd0-86e30d926323"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling add...\n",
            "{\"x\":10,\"y\":10}\n",
            "\n",
            "Calling final_answer...\n",
            "{\"answer\":\"The answer to 10 + 10 is 20\",\"tools_used\":[\"add\"]}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jPR1seluBIDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}