{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k1F5XHPGFP4",
        "outputId": "4420f513-7561-495f-895a-069008a266e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph-prebuilt 1.0.7 requires langchain-core>=1.0.0, but you have langchain-core 0.3.33 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-groq \\\n",
        "  langchain-community==0.3.16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\") or \\\n",
        "    getpass(\"Enter your GROQ API key: \")\n",
        "\n",
        "llm = ChatGroq(temperature=0.7, model=\"llama-3.3-70b-versatile\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0ikWjJBKz3-",
        "outputId": "ef191baf-4034-46ac-81ee-cefda71e76a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. ConversationBufferMemory"
      ],
      "metadata": {
        "id": "I8ETJMj2LsIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSW_nS_hLbTS",
        "outputId": "cb9e5945-eaf2-4830-c3a7-28eb16995af5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are setting return_messages=True to return the messages as a list of ChatMessage objects — unless using a non-chat model we would always set this to True as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
      ],
      "metadata": {
        "id": "pDZSnS68MPNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Hi, my name is James\"},  # user message\n",
        "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
        "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
        "    {\"output\": \"Very cool!\"}  # AI response\n",
        ")"
      ],
      "metadata": {
        "id": "Ss1HvlNDhRV_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVf16VBiifIO",
        "outputId": "3cee3232-b995-4191-9452-705726898647"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or we cn do this as well to save messages\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFAJ-VtuivZ7",
        "outputId": "5b169290-86e2-4b94-f6f3-4a56fcd4acd3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy0vdXdIjJ5M",
        "outputId": "8e3c1aed-2e67-4c8b-88c9-dd71b84754ad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-839683240.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  chain = ConversationChain(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8kA7B1Ajw9V",
        "outputId": "860f1f58-57ec-47a7-ac56-6bb5795b3b26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Your name is James, we established that at the beginning of our conversation.', additional_kwargs={}, response_metadata={})],\n",
              " 'response': 'Your name is James, we established that at the beginning of our conversation.'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationBufferMemory with RunnableWithMessageHistory  (modern way)"
      ],
      "metadata": {
        "id": "DiwLMjkxkdmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "\n",
        "system_prompt = \"You are a helpful assistant called puLeeNa.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
        "])"
      ],
      "metadata": {
        "id": "_BPWVZ-Kj9I7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = (\n",
        "    {\n",
        "        \"history\": lambda x: x[\"history\"],\n",
        "        \"query\": lambda x: x[\"query\"]\n",
        "    }\n",
        "    | prompt_template\n",
        "    | llm\n",
        ")\n",
        "\n",
        "# or use\n",
        "# pipeline = prompt_template | llm"
      ],
      "metadata": {
        "id": "m4GwyPOvlIhj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "chat_map = {}\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "  if session_id not in chat_map:\n",
        "    # if session ID doesnot exist, create a new chat history\n",
        "    chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "  return chat_map[session_id]"
      ],
      "metadata": {
        "id": "F3UJ7rjbmSoc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "iemXRHo0nt1Z"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is pulina\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NChF_JQroy5R",
        "outputId": "fbaf0992-9295-44f9-e44d-69a712a0ef75"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"What a lovely coincidence. Hi Pulina, it's nice to meet you. I'm puLeeNa, your helpful assistant. I'm here to assist you with any questions or tasks you may have. How can I help you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 52, 'total_tokens': 101, 'completion_time': 0.158504757, 'completion_tokens_details': None, 'prompt_time': 0.004980638, 'prompt_tokens_details': None, 'queue_time': 0.200397392, 'total_time': 0.163485395}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_dae98b5ecb', 'finish_reason': 'stop', 'logprobs': None}, id='run-73ede872-d24f-4c7d-8fb9-f75c8880b82c-0', usage_metadata={'input_tokens': 52, 'output_tokens': 49, 'total_tokens': 101})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name?\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlg7pu1qphBv",
        "outputId": "543881ca-2a44-4eca-c243-936f364f53fe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Pulina.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 157, 'total_tokens': 164, 'completion_time': 0.01912294, 'completion_tokens_details': None, 'prompt_time': 0.03808323, 'prompt_tokens_details': None, 'queue_time': 0.217997205, 'total_time': 0.05720617}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_c06d5113ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-c3b7f895-43f9-48fe-8257-0cd6c2b9b07a-0', usage_metadata={'input_tokens': 157, 'output_tokens': 7, 'total_tokens': 164})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# different id\n",
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name?\"},\n",
        "    config={\"session_id\": \"id_1234\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVrCq8pFpHVB",
        "outputId": "b9aafe07-86d9-420e-ee7e-bccdb5257e2e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I still don't have that information. You haven't told me your name, and I don't have the ability to know it automatically. If you'd like to share your name with me, I'd be happy to know it and use it in our conversation.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 101, 'total_tokens': 155, 'completion_time': 0.153044636, 'completion_tokens_details': None, 'prompt_time': 0.017401082, 'prompt_tokens_details': None, 'queue_time': 0.238975093, 'total_time': 0.170445718}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_c06d5113ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-03814669-a434-449d-8a4a-9f4c01415651-0', usage_metadata={'input_tokens': 101, 'output_tokens': 54, 'total_tokens': 155})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SufCPMaOpb0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "FlHZMZIrp2cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t33cT3TDp7Bw",
        "outputId": "9223a4b9-115b-4fbc-e188-f985ab57350b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3216785012.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjFbqIc8xfZ9",
        "outputId": "a8290964-21b5-4bba-c42e-af1d031eb166"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "bMKJZ58TxpOT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLO6DKX8x_vQ",
        "outputId": "1e10fdcb-c5a2-4b53-e9af-100bacf6fc6f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"I don't know your name. We just started discussing conversational memory, but you didn't mention your name in our conversation so far.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationBufferWindowMemory with RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "cePOgOQ8yYoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "  messages: list[BaseMessage] = Field(default_factory=list)\n",
        "  k: int = Field(default_factory=int)\n",
        "\n",
        "  def __init__(self, k: int):\n",
        "    super().__init__(k=k)\n",
        "    print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
        "\n",
        "  def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "    \"\"\"Add messages to the history, removing any messages beyond\n",
        "    the last `k` messages.\n",
        "    \"\"\"\n",
        "    self.messages.extend(messages)\n",
        "    self.messages = self.messages[-self.k:]\n",
        "\n",
        "  def clear(self) -> None:\n",
        "    \"\"\"Clear the message history.\"\"\"\n",
        "    self.messages = []"
      ],
      "metadata": {
        "id": "FSX1qU9SyJtz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
        "  if session_id not in chat_map:\n",
        "    chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
        "  return chat_map[session_id]"
      ],
      "metadata": {
        "id": "C0FFmk5uzIna"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the chat history\",\n",
        "            default=4,\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "8RFrBGGF3u62"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPMRUM_c5A_0",
        "outputId": "a15f8ee9-8ab1-4892-b491-551b2ea10b66"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing BufferWindowMessageHistory with k=4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello James, it's nice to meet you. I'm puLeeNa, your helpful assistant. How can I assist you today? Do you have any questions, need help with a task, or just want to chat? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 51, 'total_tokens': 102, 'completion_time': 0.108775501, 'completion_tokens_details': None, 'prompt_time': 0.005210805, 'prompt_tokens_details': None, 'queue_time': 0.196891515, 'total_time': 0.113986306}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_68f543a7cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-05bb218e-4851-411f-ba5f-14606b04e90b-0', usage_metadata={'input_tokens': 51, 'output_tokens': 51, 'total_tokens': 102})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map[\"id_k4\"].clear()  # clear the history\n",
        "\n",
        "# manually insert history\n",
        "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k4\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD8Awasz5nUI",
        "outputId": "ae73776c-7783-4625-8cb6-ead4c3745cb4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnofYVu052p8",
        "outputId": "c7b89c31-15aa-4c7a-d070-70b65ff1ce9d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"You didn't mention your name. I'm puLeeNa, by the way.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 108, 'total_tokens': 126, 'completion_time': 0.053524466, 'completion_tokens_details': None, 'prompt_time': 0.006521626, 'prompt_tokens_details': None, 'queue_time': 0.051450673, 'total_time': 0.060046092}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_bebe2dd4fb', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee0dc7fc-5c15-42c8-8980-8c8eafe53a9b-0', usage_metadata={'input_tokens': 108, 'output_tokens': 18, 'total_tokens': 126})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2e1HTxz6zgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlVPEje76JGI",
        "outputId": "202dfe8f-47ee-4db6-81ef-6bad33655b45"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing BufferWindowMessageHistory with k=14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello James, it's nice to meet you. I'm puLeeNa, your helpful assistant. How can I assist you today? Do you have any questions, need help with something, or just want to chat? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 51, 'total_tokens': 101, 'completion_time': 0.105443559, 'completion_tokens_details': None, 'prompt_time': 0.005370589, 'prompt_tokens_details': None, 'queue_time': 0.041663398, 'total_time': 0.110814148}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_bebe2dd4fb', 'finish_reason': 'stop', 'logprobs': None}, id='run-5c3f815a-316b-46a1-84f6-775205c54a33-0', usage_metadata={'input_tokens': 51, 'output_tokens': 50, 'total_tokens': 101})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k14\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izTwe9tg6mpn",
        "outputId": "7e5ee1b7-7e06-47d1-911b-546aaed38e88"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hello James, it's nice to meet you. I'm puLeeNa, your helpful assistant. How can I assist you today? Do you have any questions, need help with something, or just want to chat? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 51, 'total_tokens': 101, 'completion_time': 0.105443559, 'completion_tokens_details': None, 'prompt_time': 0.005370589, 'prompt_tokens_details': None, 'queue_time': 0.041663398, 'total_time': 0.110814148}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_bebe2dd4fb', 'finish_reason': 'stop', 'logprobs': None}, id='run-5c3f815a-316b-46a1-84f6-775205c54a33-0', usage_metadata={'input_tokens': 51, 'output_tokens': 50, 'total_tokens': 101}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name?\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFPgdWwV6wg8",
        "outputId": "a5eac234-bf18-4dda-ce79-93cbebbc8d61"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Your name is James, we established that at the beginning of our conversation. As a conversational AI, I have a type of memory that allows me to recall context and information from our conversation, similar to the ones you're researching.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 235, 'total_tokens': 283, 'completion_time': 0.252799728, 'completion_tokens_details': None, 'prompt_time': 0.015759458, 'prompt_tokens_details': None, 'queue_time': 0.014757363, 'total_time': 0.268559186}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_43d97c5965', 'finish_reason': 'stop', 'logprobs': None}, id='run-416d2338-0ac2-4056-a358-43ee31bef321-0', usage_metadata={'input_tokens': 235, 'output_tokens': 48, 'total_tokens': 283})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map[\"id_k14\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSSm8GUM64vS",
        "outputId": "9c4e4802-4309-4cbb-e5ee-56907a04b71c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hello James, it's nice to meet you. I'm puLeeNa, your helpful assistant. How can I assist you today? Do you have any questions, need help with something, or just want to chat? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 51, 'total_tokens': 101, 'completion_time': 0.105443559, 'completion_tokens_details': None, 'prompt_time': 0.005370589, 'prompt_tokens_details': None, 'queue_time': 0.041663398, 'total_time': 0.110814148}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_bebe2dd4fb', 'finish_reason': 'stop', 'logprobs': None}, id='run-5c3f815a-316b-46a1-84f6-775205c54a33-0', usage_metadata={'input_tokens': 51, 'output_tokens': 50, 'total_tokens': 101}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Your name is James, we established that at the beginning of our conversation. As a conversational AI, I have a type of memory that allows me to recall context and information from our conversation, similar to the ones you're researching.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 235, 'total_tokens': 283, 'completion_time': 0.252799728, 'completion_tokens_details': None, 'prompt_time': 0.015759458, 'prompt_tokens_details': None, 'queue_time': 0.014757363, 'total_time': 0.268559186}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_43d97c5965', 'finish_reason': 'stop', 'logprobs': None}, id='run-416d2338-0ac2-4056-a358-43ee31bef321-0', usage_metadata={'input_tokens': 235, 'output_tokens': 48, 'total_tokens': 283})]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lRszuD5V7Fzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. ConversationSummaryMemory"
      ],
      "metadata": {
        "id": "GWMwiB1G7lzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJnoWnLu7qF_",
        "outputId": "b7c03eb9-0117-4262-e640-95d52b330ea2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-988334424.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=llm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "9k16qTdr74pp"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"hello there my name is James\"})\n",
        "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
        "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
        "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
        "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29qsPsWN8B0L",
        "outputId": "0a6b77a5-3c91-4b9b-ac0d-a2b0dace2920"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: hello there my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human, James, introduces himself and the AI responds by introducing itself as a conversational AI, explaining its training and capabilities, and expressing excitement to chat with James, inquiring about his interests and the purpose of his visit.\n",
            "Human: I am researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human, James, introduces himself and the AI responds by introducing itself as a conversational AI, explaining its training and capabilities, and expressing excitement to chat with James, inquiring about his interests and the purpose of his visit. James is researching the different types of conversational memory, and the AI is happy to help, explaining that it utilizes a combination of episodic, semantic, and procedural memory to engage in discussions, and also employs working memory to process and respond to user input in real-time, and now the AI is inquiring about James' specific research interests, whether he is focusing on the cognitive aspects of human conversation or the technical implementations in AI systems.\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human, James, introduces himself and the AI responds by introducing itself as a conversational AI, explaining its training and capabilities, and expressing excitement to chat with James, inquiring about his interests and the purpose of his visit. James is researching the different types of conversational memory, and the AI is happy to help, explaining that it utilizes a combination of episodic, semantic, and procedural memory to engage in discussions, and also employs working memory to process and respond to user input in real-time, and now the AI is inquiring about James' specific research interests, whether he is focusing on the cognitive aspects of human conversation or the technical implementations in AI systems. James is specifically looking at ConversationBufferMemory and ConversationBufferWindowMemory, which the AI explains are two types of memory architectures used in conversational AI systems, with ConversationBufferMemory storing the context of a conversation, including the history of user input and system responses, and ConversationBufferWindowMemory storing a window of recent conversation history to improve response relevance and reduce computational overhead. The AI uses a variant of ConversationBufferMemory and has been trained on a dataset that utilizes a dynamic window approach for ConversationBufferWindowMemory, and is now inquiring about James' research interests in these areas, including potential explorations of the trade-offs between using ConversationBufferMemory versus ConversationBufferWindowMemory, or investigations into how these memory architectures impact the overall performance and user experience of conversational AI systems.\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human, James, introduces himself and the AI responds by introducing itself as a conversational AI, explaining its training and capabilities, and expressing excitement to chat with James, inquiring about his interests and the purpose of his visit. James is researching the different types of conversational memory, and the AI is happy to help, explaining that it utilizes a combination of episodic, semantic, and procedural memory to engage in discussions, and also employs working memory to process and respond to user input in real-time, and now the AI is inquiring about James' specific research interests, whether he is focusing on the cognitive aspects of human conversation or the technical implementations in AI systems. James is specifically looking at ConversationBufferMemory and ConversationBufferWindowMemory, which the AI explains are two types of memory architectures used in conversational AI systems, with ConversationBufferMemory storing the context of a conversation, including the history of user input and system responses, and ConversationBufferWindowMemory storing a window of recent conversation history to improve response relevance and reduce computational overhead. The AI uses a variant of ConversationBufferMemory and has been trained on a dataset that utilizes a dynamic window approach for ConversationBufferWindowMemory, and is now inquiring about James' research interests in these areas, including potential explorations of the trade-offs between using ConversationBufferMemory versus ConversationBufferWindowMemory, or investigations into how these memory architectures impact the overall performance and user experience of conversational AI systems. James notes that ConversationBufferMemory stores the entire conversation history, and the AI agrees, adding that while this approach can be useful for maintaining context, it can also lead to increased computational overhead, whereas ConversationBufferWindowMemory stores only a limited window of recent conversation history, reducing overhead but potentially limiting recall of earlier interactions, and the AI is now exploring with James the trade-offs between these two approaches, including the benefits and drawbacks of each, such as improved context awareness versus increased computational overhead, and how these impact the performance and user experience of conversational AI systems.\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
              " 'history': \"The human, James, introduces himself and the AI responds by introducing itself as a conversational AI, explaining its training and capabilities, and expressing excitement to chat with James, inquiring about his interests and the purpose of his visit. James is researching the different types of conversational memory, and the AI is happy to help, explaining that it utilizes a combination of episodic, semantic, and procedural memory to engage in discussions, and also employs working memory to process and respond to user input in real-time, and now the AI is inquiring about James' specific research interests, whether he is focusing on the cognitive aspects of human conversation or the technical implementations in AI systems. James is specifically looking at ConversationBufferMemory and ConversationBufferWindowMemory, which the AI explains are two types of memory architectures used in conversational AI systems, with ConversationBufferMemory storing the context of a conversation, including the history of user input and system responses, and ConversationBufferWindowMemory storing a window of recent conversation history to improve response relevance and reduce computational overhead. The AI uses a variant of ConversationBufferMemory and has been trained on a dataset that utilizes a dynamic window approach for ConversationBufferWindowMemory, and is now inquiring about James' research interests in these areas, including potential explorations of the trade-offs between using ConversationBufferMemory versus ConversationBufferWindowMemory, or investigations into how these memory architectures impact the overall performance and user experience of conversational AI systems. James notes that ConversationBufferMemory stores the entire conversation history, and the AI agrees, adding that while this approach can be useful for maintaining context, it can also lead to increased computational overhead, whereas ConversationBufferWindowMemory stores only a limited window of recent conversation history, reducing overhead but potentially limiting recall of earlier interactions, and the AI is now exploring with James the trade-offs between these two approaches, including the benefits and drawbacks of each, such as improved context awareness versus increased computational overhead, and how these impact the performance and user experience of conversational AI systems.\",\n",
              " 'response': \"That's a great point, James. ConversationBufferWindowMemory indeed stores the last k messages, effectively creating a sliding window of recent conversation history. This approach allows for efficient use of computational resources, as it only considers a fixed number of previous messages when generating responses. The value of k can be tuned to balance the trade-off between recall of earlier interactions and computational overhead. In my own implementation, I use a dynamic window approach, where the value of k is adjusted based on the conversation's context and the user's engagement. This allows me to adapt to changing conversation dynamics and optimize my responses accordingly. I'm curious, James, have you explored the impact of different k values on the performance of conversational AI systems, and how they affect the user experience? For instance, do you think a larger k value would lead to more accurate responses, but at the cost of increased latency, or would a smaller k value result in more efficient processing, but potentially at the expense of context awareness?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"what is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3o8ydEV8EMu",
        "outputId": "4d294600-fc87-4987-9b38-a6bb07c94176"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human, James, introduces himself and the AI responds by introducing itself as a conversational AI, explaining its training and capabilities, and expressing excitement to chat with James, inquiring about his interests and the purpose of his visit. James is researching the different types of conversational memory, and the AI is happy to help, explaining that it utilizes a combination of episodic, semantic, and procedural memory to engage in discussions, and also employs working memory to process and respond to user input in real-time, and now the AI is inquiring about James' specific research interests, whether he is focusing on the cognitive aspects of human conversation or the technical implementations in AI systems. James is specifically looking at ConversationBufferMemory and ConversationBufferWindowMemory, which the AI explains are two types of memory architectures used in conversational AI systems, with ConversationBufferMemory storing the context of a conversation, including the history of user input and system responses, and ConversationBufferWindowMemory storing a window of recent conversation history to improve response relevance and reduce computational overhead. The AI uses a variant of ConversationBufferMemory and has been trained on a dataset that utilizes a dynamic window approach for ConversationBufferWindowMemory, and is now inquiring about James' research interests in these areas, including potential explorations of the trade-offs between using ConversationBufferMemory versus ConversationBufferWindowMemory, or investigations into how these memory architectures impact the overall performance and user experience of conversational AI systems. James notes that ConversationBufferMemory stores the entire conversation history, and the AI agrees, adding that while this approach can be useful for maintaining context, it can also lead to increased computational overhead, whereas ConversationBufferWindowMemory stores only a limited window of recent conversation history, reducing overhead but potentially limiting recall of earlier interactions. The AI explores with James the trade-offs between these two approaches, including the benefits and drawbacks of each, such as improved context awareness versus increased computational overhead, and how these impact the performance and user experience of conversational AI systems. James mentions that ConversationBufferWindowMemory stores the last k messages, dropping the rest, and the AI explains that this approach allows for efficient use of computational resources, with the value of k being tunable to balance the trade-off between recall of earlier interactions and computational overhead. The AI uses a dynamic window approach, adjusting the value of k based on the conversation's context and the user's engagement, and is now inquiring about James' thoughts on the impact of different k values on the performance of conversational AI systems, and how they affect the user experience, such as the potential trade-offs between response accuracy and latency.\n",
            "Human: what is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is my name?',\n",
              " 'history': \"The human, James, introduces himself and the AI responds by introducing itself as a conversational AI, explaining its training and capabilities, and expressing excitement to chat with James, inquiring about his interests and the purpose of his visit. James is researching the different types of conversational memory, and the AI is happy to help, explaining that it utilizes a combination of episodic, semantic, and procedural memory to engage in discussions, and also employs working memory to process and respond to user input in real-time, and now the AI is inquiring about James' specific research interests, whether he is focusing on the cognitive aspects of human conversation or the technical implementations in AI systems. James is specifically looking at ConversationBufferMemory and ConversationBufferWindowMemory, which the AI explains are two types of memory architectures used in conversational AI systems, with ConversationBufferMemory storing the context of a conversation, including the history of user input and system responses, and ConversationBufferWindowMemory storing a window of recent conversation history to improve response relevance and reduce computational overhead. The AI uses a variant of ConversationBufferMemory and has been trained on a dataset that utilizes a dynamic window approach for ConversationBufferWindowMemory, and is now inquiring about James' research interests in these areas, including potential explorations of the trade-offs between using ConversationBufferMemory versus ConversationBufferWindowMemory, or investigations into how these memory architectures impact the overall performance and user experience of conversational AI systems. James notes that ConversationBufferMemory stores the entire conversation history, and the AI agrees, adding that while this approach can be useful for maintaining context, it can also lead to increased computational overhead, whereas ConversationBufferWindowMemory stores only a limited window of recent conversation history, reducing overhead but potentially limiting recall of earlier interactions. The AI explores with James the trade-offs between these two approaches, including the benefits and drawbacks of each, such as improved context awareness versus increased computational overhead, and how these impact the performance and user experience of conversational AI systems. James mentions that ConversationBufferWindowMemory stores the last k messages, dropping the rest, and the AI explains that this approach allows for efficient use of computational resources, with the value of k being tunable to balance the trade-off between recall of earlier interactions and computational overhead. The AI uses a dynamic window approach, adjusting the value of k based on the conversation's context and the user's engagement, and is now inquiring about James' thoughts on the impact of different k values on the performance of conversational AI systems, and how they affect the user experience, such as the potential trade-offs between response accuracy and latency.\",\n",
              " 'response': \"Your name is James. We've been having a great conversation so far about conversational memory, specifically ConversationBufferMemory and ConversationBufferWindowMemory. Would you like to continue exploring those topics or is there something else you'd like to discuss?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationSummaryMemory with RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "rW-Pigzk86o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "  messages: list[BaseMessage] = Field(default_factory=list)\n",
        "  llm: ChatGroq = Field(default_factory=ChatGroq)\n",
        "\n",
        "  def __init__(self, llm: ChatGroq):\n",
        "    super().__init__(llm=llm)\n",
        "\n",
        "  def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "    \"\"\"Create a summary.\n",
        "    \"\"\"\n",
        "    self.messages.extend(messages)\n",
        "    summary_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"Given the existing conversation summary and the new messages, \"\n",
        "            \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "            \"as much relevant information as possible.\"\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\n",
        "            \"Existing summary:\\n{existing_summary}\\n\\n\"\n",
        "            \"New messages:\\n{messages}\"\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    new_summary = self.llm.invoke(\n",
        "        summary_prompt.format_messages(existing_summary=self.messages, messages=[x.content for x in messages])\n",
        "    )\n",
        "\n",
        "    self.messages = [SystemMessage(content=new_summary.content)]\n",
        "\n",
        "  def clear(self) -> None:\n",
        "    \"\"\"Clear the message history.\"\"\"\n",
        "    self.messages = []"
      ],
      "metadata": {
        "id": "sfdnTQpG80df"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatGroq) -> ConversationSummaryMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ],
      "metadata": {
        "id": "D1qsq22uA_C2"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatGroq,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "a9qUkSdDBIyk"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGwIm7yYBOo9",
        "outputId": "a5cc2f40-ba8e-4a73-ba17-86188b50ccb0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello James, it's nice to meet you. I'm puLeeNa, your helpful assistant. How can I assist you today? Do you have any questions, need help with something, or just want to chat? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 51, 'total_tokens': 101, 'completion_time': 0.119640403, 'completion_tokens_details': None, 'prompt_time': 0.002442006, 'prompt_tokens_details': None, 'queue_time': 0.008501461, 'total_time': 0.122082409}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_43d97c5965', 'finish_reason': 'stop', 'logprobs': None}, id='run-637c1886-fb6b-492a-ba80-8b995b562f1e-0', usage_metadata={'input_tokens': 51, 'output_tokens': 50, 'total_tokens': 101})"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map[\"id_123\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euTOxuuiBhPI",
        "outputId": "9156c94f-c920-4ba5-88de-97eea0efffe8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"New Summary:\\nThe conversation starts with James introducing himself. puLeeNa, the helpful assistant, responds with a greeting and offers assistance, inquiring if James has any questions, needs help, or wants to chat. The conversation is ongoing, with puLeeNa waiting for James' response. \\n\\nKey points: \\n- James introduced himself.\\n- puLeeNa greeted James and offered assistance.\\n- The conversation topic has not been determined yet. \\n\\nThe conversation is in its initial stages, and the direction it will take depends on James' next message.\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")\n",
        "\n",
        "chat_map[\"id_123\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXWqtrktBpXt",
        "outputId": "87ac8a97-70df-466d-b9f7-a1c5d8a51969"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"New Summary:\\nThe conversation starts with James introducing himself and researching different types of conversational memory. puLeeNa, the helpful assistant, responds with a greeting, offers assistance, and provides an in-depth explanation of conversational memory, including its types: episodic, semantic, procedural, working, and long-term memory. puLeeNa also inquires about the specific aspect of conversational memory James is researching, whether it's related to human conversation or artificial intelligence applications.\\n\\nKey points:\\n- James introduced himself and is researching conversational memory.\\n- puLeeNa greeted James, offered assistance, and explained the types of conversational memory.\\n- The types of conversational memory include episodic, semantic, procedural, working, and long-term memory.\\n- puLeeNa asked James about the specific aspect of conversational memory he is researching.\\n- The conversation is exploring the topic of conversational memory, with a potential focus on its applications in humans or artificial intelligence.\\n\\nThe conversation is progressing, with puLeeNa providing information and seeking clarification on James' research interests. The direction it will take depends on James' next message, which may delve deeper into the specifics of conversational memory or explore its applications in various fields.\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for msg in [\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        "    )"
      ],
      "metadata": {
        "id": "TuDUPzu5CQWS"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map[\"id_123\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv9qJJ57CeUB",
        "outputId": "2695279d-b27d-42fb-c8a6-9b43bd3b64ce"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"New Summary:\\nThe conversation begins with James introducing himself and researching different types of conversational memory. puLeeNa, the helpful assistant, responds with a greeting, offers assistance, and provides an in-depth explanation of conversational memory, including its types: episodic, semantic, procedural, working, and long-term memory. James then specifies that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. puLeeNa explains that these are specific types of conversational memory models related to buffering and windowing in conversational AI.\\n\\nJames points out that buffer memory simply stores the entire conversation, which puLeeNa confirms. puLeeNa elaborates that ConversationBufferMemory allows the model to keep track of all previous turns and context, enabling it to capture long-range dependencies and relationships between different parts of the conversation. This can be useful in applications where understanding the entire conversation history is crucial.\\n\\nIn contrast, ConversationBufferWindowMemory focuses on a fixed-size window of previous conversation turns, which can be more efficient in terms of computational resources and memory usage but may not capture the entire conversation history. James notes that Buffer window memory stores the last k messages, dropping the rest. puLeeNa expands on this, explaining that by storing only the last k messages, ConversationBufferWindowMemory creates a sliding window of context that the model can draw upon to understand the current conversation turn. This approach has several benefits, including efficient use of resources, focus on recent context, and reducing noise.\\n\\nHowever, as James mentioned earlier, this approach may not be suitable for all applications, especially those that require understanding the entire conversation history. puLeeNa inquires about James' thoughts on the trade-offs between storing the entire conversation versus using a window-based approach and asks if there are specific use cases or applications where one approach might be more suitable than the other. Additionally, puLeeNa asks for James' thoughts on the value of k in ConversationBufferWindowMemory, wondering if it should be a fixed value or dynamically adjusted based on the conversation flow or other factors.\\n\\nKey points:\\n- James introduced himself and is researching conversational memory, specifically ConversationBufferMemory and ConversationBufferWindowMemory.\\n- puLeeNa greeted James, offered assistance, and explained the types of conversational memory, including episodic, semantic, procedural, working, and long-term memory.\\n- ConversationBufferMemory stores the entire conversation, allowing the model to understand the conversation flow and generate coherent responses.\\n- ConversationBufferWindowMemory focuses on a fixed-size window of previous conversation turns, helping the model to capture contextual information and generate accurate responses.\\n- James and puLeeNa discussed the trade-offs between storing the entire conversation versus using a window-based approach, considering factors like computational resources, memory usage, and application-specific requirements.\\n- puLeeNa seeks James' input on potential use cases or applications where one approach might be more suitable than the other and asks about the value of k in ConversationBufferWindowMemory, guiding the conversation towards exploring practical implications of conversational memory models.\\n\\nThe conversation is progressing, with James and puLeeNa delving deeper into the specifics of ConversationBufferMemory and ConversationBufferWindowMemory, including their strengths, limitations, and potential applications. The direction it will take depends on James' next message, which may provide more insights into his research interests or explore the practical implications of these conversational memory models.\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "        {\"query\": \"what is my name?\"},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbVxxZcxCoFt",
        "outputId": "232fb169-2e6d-4cdd-de85-ea89c5810909"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 722, 'total_tokens': 728, 'completion_time': 0.02387185, 'completion_tokens_details': None, 'prompt_time': 0.086660956, 'prompt_tokens_details': None, 'queue_time': 0.039143167, 'total_time': 0.110532806}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_bebe2dd4fb', 'finish_reason': 'stop', 'logprobs': None}, id='run-0c2965d2-31d9-4511-8750-7d05ade9c707-0', usage_metadata={'input_tokens': 722, 'output_tokens': 6, 'total_tokens': 728})"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eKo41T_hCwa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. ConversationSummaryBufferMemory"
      ],
      "metadata": {
        "id": "fE4NN_uaDTwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatGroq = Field(default_factory=ChatGroq)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, llm: ChatGroq, k: int):\n",
        "        super().__init__(llm=llm, k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages and summarizing the messages that we\n",
        "        drop.\n",
        "        \"\"\"\n",
        "        existing_summary: SystemMessage | None = None\n",
        "        old_messages: list[BaseMessage] | None = None\n",
        "        # see if we already have a summary message\n",
        "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "            print(\">> Found existing summary\")\n",
        "            existing_summary = self.messages.pop(0)\n",
        "        # add the new messages to the history\n",
        "        self.messages.extend(messages)\n",
        "        # check if we have too many messages\n",
        "        if len(self.messages) > self.k:\n",
        "            print(\n",
        "                f\">> Found {len(self.messages)} messages, dropping \"\n",
        "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
        "            # pull out the oldest messages...\n",
        "            old_messages = self.messages[:-self.k]\n",
        "            # ...and keep only the most recent messages\n",
        "            self.messages = self.messages[-self.k:]\n",
        "        if old_messages is None:\n",
        "            print(\">> No old messages to update summary with\")\n",
        "            # if we have no old_messages, we have nothing to update in summary\n",
        "            return\n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{old_messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=existing_summary,\n",
        "                old_messages=old_messages\n",
        "            )\n",
        "        )\n",
        "        print(f\">> New summary: {new_summary.content}\")\n",
        "        # prepend the new summary to the history\n",
        "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ],
      "metadata": {
        "id": "lu4VB6L1Dsfj"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatGroq, k: int) -> ConversationSummaryBufferMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ],
      "metadata": {
        "id": "wLNLwYe8IK7f"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatGroq,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Ch4DPsEMJbmE"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        ")\n",
        "chat_map[\"id_123\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPVZ8xz1JiNV",
        "outputId": "1b34e136-6307-46dc-cbfc-e28f7da371ee"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> No old messages to update summary with\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hello James, it's nice to meet you. I'm puLeeNa, your helpful assistant. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 51, 'total_tokens': 87, 'completion_time': 0.086775489, 'completion_tokens_details': None, 'prompt_time': 0.001720926, 'prompt_tokens_details': None, 'queue_time': 0.008819334, 'total_time': 0.088496415}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_43d97c5965', 'finish_reason': 'stop', 'logprobs': None}, id='run-39fa6161-45a9-467a-abc7-cd6f0a67a420-0', usage_metadata={'input_tokens': 51, 'output_tokens': 36, 'total_tokens': 87})]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, msg in enumerate([\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]):\n",
        "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YpQwwDcJrAX",
        "outputId": "56f95c3b-cf4c-487a-a65e-1d543e919412"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "Message 1\n",
            "---\n",
            "\n",
            ">> No old messages to update summary with\n",
            "---\n",
            "Message 2\n",
            "---\n",
            "\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: Here is a new summary of the conversation:\n",
            "\n",
            "The conversation started with James introducing himself. The assistant, puLeeNa, responded with a greeting and offered to help James with any questions or topics he'd like to discuss. The conversation has just begun, and James has not yet responded to puLeeNa's offer.\n",
            "---\n",
            "Message 3\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: Here is a new summary of the conversation:\n",
            "\n",
            "The conversation started with James introducing himself. The assistant, puLeeNa, responded with a greeting and offered to help James with any questions or topics he'd like to discuss. James then expressed his interest in researching the different types of conversational memory. The assistant provided an overview of the various types of conversational memory, including episodic, semantic, procedural, working, and long-term memory, as well as techniques used in conversational AI, such as contextual understanding, conversation history, and entity recognition. The assistant has asked James to specify which aspects of conversational memory he is researching.\n",
            "---\n",
            "Message 4\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: Here is a new summary of the conversation:\n",
            "\n",
            "The conversation started with James introducing himself. The assistant, puLeeNa, responded with a greeting and offered to help James with any questions or topics he'd like to discuss. James then expressed his interest in researching the different types of conversational memory. The assistant provided an overview of the various types of conversational memory, including episodic, semantic, procedural, working, and long-term memory, as well as techniques used in conversational AI, such as contextual understanding, conversation history, and entity recognition.\n",
            "\n",
            "James specified that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The assistant explained that these are two specific types of conversational memory models, with ConversationBufferMemory storing a buffer of recent conversation history and ConversationBufferWindowMemory using a sliding window approach to store and retrieve conversation history. The assistant discussed the limitations of these models, including limited context and lack of long-term memory, and mentioned more advanced conversational memory models, such as graph-based memory models and attention-based memory models.\n",
            "\n",
            "The conversation is currently focused on James' thoughts on ConversationBufferMemory and ConversationBufferWindowMemory, and whether he is exploring any specific applications or use cases for these models. The assistant is awaiting James' response to continue the discussion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "imzxvtKqJ25-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
