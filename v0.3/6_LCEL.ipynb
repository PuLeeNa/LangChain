{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6LEgFzMOQJd",
        "outputId": "8b7437d9-d1cf-41c8-cb08-50b3ab934bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-huggingface 1.2.0 requires langchain-core<2.0.0,>=1.2.0, but you have langchain-core 0.3.33 which is incompatible.\n",
            "langgraph-prebuilt 1.0.7 requires langchain-core>=1.0.0, but you have langchain-core 0.3.33 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-groq \\\n",
        "  langchain-community==0.3.16 \\\n",
        "  docarray==0.40.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIIiVAJ-YEcV",
        "outputId": "4e63d4ff-59fa-4bfc-e193-0687c7efbad0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/566.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m563.2/566.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.3/566.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/495.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.8/495.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-groq 0.2.4 requires langchain-core<0.4.0,>=0.3.33, but you have langchain-core 1.2.8 which is incompatible.\n",
            "langchain-text-splitters 0.3.5 requires langchain-core<0.4.0,>=0.3.29, but you have langchain-core 1.2.8 which is incompatible.\n",
            "langchain-community 0.3.16 requires langchain-core<0.4.0,>=0.3.32, but you have langchain-core 1.2.8 which is incompatible.\n",
            "langchain 0.3.17 requires langchain-core<0.4.0,>=0.3.33, but you have langchain-core 1.2.8 which is incompatible.\n",
            "transformers 5.0.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.36.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traditional LLMChain"
      ],
      "metadata": {
        "id": "405P3M2OOgaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\") or \\\n",
        "    getpass(\"Enter your GROQ API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fJwVbUYOdd6",
        "outputId": "37248648-3815-4485-ebab-294cf7b2d7c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(temperature=0.0, model=\"llama-3.3-70b-versatile\")"
      ],
      "metadata": {
        "id": "f-KNpmfeO7AW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt_template = \"Give me a small report on {topic}\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "oD2YbNQUOzTB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_out = llm.invoke(\"Hello there\")\n",
        "llm_out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtV1cDW4Pela",
        "outputId": "352e9cf4-d439-40d9-be3c-f63ae5c4ace9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 37, 'total_tokens': 62, 'completion_time': 0.058544326, 'completion_tokens_details': None, 'prompt_time': 0.002234469, 'prompt_tokens_details': None, 'queue_time': 0.039186593, 'total_time': 0.060778795}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_43d97c5965', 'finish_reason': 'stop', 'logprobs': None}, id='run-e50cc2de-3a9e-47c8-802c-2ac9de5b3187-0', usage_metadata={'input_tokens': 37, 'output_tokens': 25, 'total_tokens': 62})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the **StrOutputParser** which will parse the AIMessage output from our LLM into a single string."
      ],
      "metadata": {
        "id": "HymSp97wP3FM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "5z95uwe3O6BR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = output_parser.invoke(llm_out)"
      ],
      "metadata": {
        "id": "XjTbb11MPRYK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XbHLKUKTPjOZ",
        "outputId": "9e7e15be-e5d2-4cd7-de9e-e79cf1b1d7bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the LLMChain class we can place each of our components into a linear chain"
      ],
      "metadata": {
        "id": "lhpc8QX0QPU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
      ],
      "metadata": {
        "id": "rMGCmT-hPk-f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke(\"retrieval augmented generation\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pih8RO89QeTP",
        "outputId": "023d257a-6be9-42fe-f643-361b509d7f68"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'retrieval augmented generation',\n",
              " 'text': '**Retrieval Augmented Generation: A New Paradigm in Natural Language Processing**\\n\\nRetrieval Augmented Generation (RAG) is a novel approach in Natural Language Processing (NLP) that combines the strengths of retrieval-based and generation-based models. The goal of RAG is to improve the accuracy, efficiency, and diversity of text generation tasks, such as question answering, text summarization, and dialogue systems.\\n\\n**Key Components:**\\n\\n1. **Retrieval Module:** This module retrieves relevant information from a large corpus of text, such as a knowledge graph or a database. The retrieval module uses techniques like keyword extraction, entity recognition, and semantic search to identify relevant passages or documents.\\n2. **Generation Module:** This module generates text based on the retrieved information. The generation module uses techniques like language modeling, machine translation, and text summarization to produce coherent and contextually relevant text.\\n\\n**How RAG Works:**\\n\\n1. The retrieval module retrieves relevant information from the corpus based on the input prompt or query.\\n2. The generation module uses the retrieved information to generate text that is contextually relevant and accurate.\\n3. The generated text is then post-processed to ensure coherence, fluency, and grammatical correctness.\\n\\n**Benefits:**\\n\\n1. **Improved Accuracy:** RAG can improve the accuracy of text generation tasks by leveraging the strengths of both retrieval-based and generation-based models.\\n2. **Increased Efficiency:** RAG can reduce the computational requirements of text generation tasks by retrieving relevant information from a large corpus, rather than generating text from scratch.\\n3. **Enhanced Diversity:** RAG can generate more diverse and contextually relevant text by combining the strengths of retrieval-based and generation-based models.\\n\\n**Applications:**\\n\\n1. **Question Answering:** RAG can be used to improve the accuracy of question answering systems by retrieving relevant information from a large corpus and generating contextually relevant answers.\\n2. **Text Summarization:** RAG can be used to improve the accuracy of text summarization systems by retrieving relevant information from a large corpus and generating concise and contextually relevant summaries.\\n3. **Dialogue Systems:** RAG can be used to improve the accuracy and diversity of dialogue systems by retrieving relevant information from a large corpus and generating contextually relevant responses.\\n\\n**Conclusion:**\\n\\nRetrieval Augmented Generation is a promising approach in NLP that combines the strengths of retrieval-based and generation-based models. By leveraging the strengths of both approaches, RAG can improve the accuracy, efficiency, and diversity of text generation tasks, with applications in question answering, text summarization, and dialogue systems.'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(result[\"text\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "hZZERr0RQwxR",
        "outputId": "e8de2d03-77be-4bae-eaea-90ddb4cd1112"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Retrieval Augmented Generation: A New Paradigm in Natural Language Processing**\n\nRetrieval Augmented Generation (RAG) is a novel approach in Natural Language Processing (NLP) that combines the strengths of retrieval-based and generation-based models. The goal of RAG is to improve the accuracy, efficiency, and diversity of text generation tasks, such as question answering, text summarization, and dialogue systems.\n\n**Key Components:**\n\n1. **Retrieval Module:** This module retrieves relevant information from a large corpus of text, such as a knowledge graph or a database. The retrieval module uses techniques like keyword extraction, entity recognition, and semantic search to identify relevant passages or documents.\n2. **Generation Module:** This module generates text based on the retrieved information. The generation module uses techniques like language modeling, machine translation, and text summarization to produce coherent and contextually relevant text.\n\n**How RAG Works:**\n\n1. The retrieval module retrieves relevant information from the corpus based on the input prompt or query.\n2. The generation module uses the retrieved information to generate text that is contextually relevant and accurate.\n3. The generated text is then post-processed to ensure coherence, fluency, and grammatical correctness.\n\n**Benefits:**\n\n1. **Improved Accuracy:** RAG can improve the accuracy of text generation tasks by leveraging the strengths of both retrieval-based and generation-based models.\n2. **Increased Efficiency:** RAG can reduce the computational requirements of text generation tasks by retrieving relevant information from a large corpus, rather than generating text from scratch.\n3. **Enhanced Diversity:** RAG can generate more diverse and contextually relevant text by combining the strengths of retrieval-based and generation-based models.\n\n**Applications:**\n\n1. **Question Answering:** RAG can be used to improve the accuracy of question answering systems by retrieving relevant information from a large corpus and generating contextually relevant answers.\n2. **Text Summarization:** RAG can be used to improve the accuracy of text summarization systems by retrieving relevant information from a large corpus and generating concise and contextually relevant summaries.\n3. **Dialogue Systems:** RAG can be used to improve the accuracy and diversity of dialogue systems by retrieving relevant information from a large corpus and generating contextually relevant responses.\n\n**Conclusion:**\n\nRetrieval Augmented Generation is a promising approach in NLP that combines the strengths of retrieval-based and generation-based models. By leveraging the strengths of both approaches, RAG can improve the accuracy, efficiency, and diversity of text generation tasks, with applications in question answering, text summarization, and dialogue systems."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AgCWJAGWRAGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Expression Language (LCEL)"
      ],
      "metadata": {
        "id": "XquqV3aPRax5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lcel_chain = prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "VcNSACodRcZD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "Nrr1QeJsSwqv",
        "outputId": "fa9095ec-78e4-4de4-8eea-cbe9a62f2e59"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Retrieval Augmented Generation: A New Paradigm in Natural Language Processing**\\n\\nRetrieval Augmented Generation (RAG) is a novel approach in Natural Language Processing (NLP) that combines the strengths of retrieval-based and generation-based models. The goal of RAG is to improve the accuracy, efficiency, and diversity of text generation tasks, such as question answering, text summarization, and dialogue systems.\\n\\n**Key Components:**\\n\\n1. **Retrieval Module:** This module retrieves relevant information from a large corpus of text, such as a knowledge graph or a database. The retrieval module uses techniques like keyword extraction, entity recognition, and semantic search to identify relevant passages or documents.\\n2. **Generation Module:** This module generates text based on the retrieved information. The generation module uses techniques like language modeling, machine translation, and text summarization to produce coherent and contextually relevant text.\\n\\n**How RAG Works:**\\n\\n1. The retrieval module retrieves relevant information from the corpus based on the input prompt or query.\\n2. The generation module uses the retrieved information to generate text that is contextually relevant and accurate.\\n3. The generated text is then post-processed to ensure coherence, fluency, and grammatical correctness.\\n\\n**Benefits:**\\n\\n1. **Improved Accuracy:** RAG can improve the accuracy of text generation tasks by leveraging the strengths of both retrieval-based and generation-based models.\\n2. **Increased Efficiency:** RAG can reduce the computational requirements of text generation tasks by retrieving relevant information from a large corpus, rather than generating text from scratch.\\n3. **Enhanced Diversity:** RAG can generate more diverse and contextually relevant text by combining the strengths of retrieval-based and generation-based models.\\n\\n**Applications:**\\n\\n1. **Question Answering:** RAG can be used to improve the accuracy of question answering systems by retrieving relevant information from a large corpus and generating contextually relevant answers.\\n2. **Text Summarization:** RAG can be used to improve the accuracy of text summarization systems by retrieving relevant information from a large corpus and generating concise and contextually relevant summaries.\\n3. **Dialogue Systems:** RAG can be used to improve the accuracy and diversity of dialogue systems by retrieving relevant information from a large corpus and generating contextually relevant responses.\\n\\n**Conclusion:**\\n\\nRetrieval Augmented Generation is a promising approach in NLP that combines the strengths of retrieval-based and generation-based models. By leveraging the strengths of both approaches, RAG can improve the accuracy, efficiency, and diversity of text generation tasks, with applications in question answering, text summarization, and dialogue systems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "oNKw1Q03S0hX",
        "outputId": "df6fdb76-0452-4ce5-a99f-98d0b879ac84"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Retrieval Augmented Generation: A New Paradigm in Natural Language Processing**\n\nRetrieval Augmented Generation (RAG) is a novel approach in Natural Language Processing (NLP) that combines the strengths of retrieval-based and generation-based models. The goal of RAG is to improve the accuracy, efficiency, and diversity of text generation tasks, such as question answering, text summarization, and dialogue systems.\n\n**Key Components:**\n\n1. **Retrieval Module:** This module retrieves relevant information from a large corpus of text, such as a knowledge graph or a database. The retrieval module uses techniques like keyword extraction, entity recognition, and semantic search to identify relevant passages or documents.\n2. **Generation Module:** This module generates text based on the retrieved information. The generation module uses techniques like language modeling, machine translation, and text summarization to produce coherent and contextually relevant text.\n\n**How RAG Works:**\n\n1. The retrieval module retrieves relevant information from the corpus based on the input prompt or query.\n2. The generation module uses the retrieved information to generate text that is contextually relevant and accurate.\n3. The generated text is then post-processed to ensure coherence, fluency, and grammatical correctness.\n\n**Benefits:**\n\n1. **Improved Accuracy:** RAG can improve the accuracy of text generation tasks by leveraging the strengths of both retrieval-based and generation-based models.\n2. **Increased Efficiency:** RAG can reduce the computational requirements of text generation tasks by retrieving relevant information from a large corpus, rather than generating text from scratch.\n3. **Enhanced Diversity:** RAG can generate more diverse and contextually relevant text by combining the strengths of retrieval-based and generation-based models.\n\n**Applications:**\n\n1. **Question Answering:** RAG can be used to improve the accuracy of question answering systems by retrieving relevant information from a large corpus and generating contextually relevant answers.\n2. **Text Summarization:** RAG can be used to improve the accuracy of text summarization systems by retrieving relevant information from a large corpus and generating concise and contextually relevant summaries.\n3. **Dialogue Systems:** RAG can be used to improve the accuracy and diversity of dialogue systems by retrieving relevant information from a large corpus and generating contextually relevant responses.\n\n**Conclusion:**\n\nRetrieval Augmented Generation is a promising approach in NLP that combines the strengths of retrieval-based and generation-based models. By leveraging the strengths of both approaches, RAG can improve the accuracy, efficiency, and diversity of text generation tasks, with applications in question answering, text summarization, and dialogue systems."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LCEL RunnableLambda"
      ],
      "metadata": {
        "id": "09rssZJ7UVX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **RunnableLambda** class is LangChain's built-in method for constructing a runnable object from a function."
      ],
      "metadata": {
        "id": "PqvThlgvUvSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_five(x):\n",
        "    return x+5\n",
        "\n",
        "def sub_five(x):\n",
        "    return x-5\n",
        "\n",
        "def mul_five(x):\n",
        "    return x*5"
      ],
      "metadata": {
        "id": "LCTcY544TXIE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "add_five_runnable = RunnableLambda(add_five)\n",
        "sub_five_runnable = RunnableLambda(sub_five)\n",
        "mul_five_runnable = RunnableLambda(mul_five)"
      ],
      "metadata": {
        "id": "eN7Rat8CUuS1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
      ],
      "metadata": {
        "id": "JzGz1T_yU5uS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfyLlntEU74r",
        "outputId": "5bcf8b0c-6d67-43d8-d29a-ac017dac9172"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VIIzVsirU9vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_str = \"give me a small report about {topic}\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_str\n",
        ")"
      ],
      "metadata": {
        "id": "8vNDXaAdVM0T"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "oT4Fjc84VPAW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke(\"AI\")\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "V9YzPOErVRDJ",
        "outputId": "ff901f42-7414-4c66-a274-6ea0f6206be7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Artificial Intelligence (AI) Report**\n\n**Introduction:**\nArtificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. AI has been rapidly advancing in recent years, with significant breakthroughs in areas like machine learning, natural language processing, and computer vision.\n\n**Key Developments:**\n\n1. **Machine Learning:** AI algorithms can now learn from large datasets, enabling applications like image recognition, speech recognition, and predictive analytics.\n2. **Natural Language Processing (NLP):** AI-powered NLP has improved significantly, allowing for more accurate language translation, sentiment analysis, and text summarization.\n3. **Computer Vision:** AI-powered computer vision has enabled applications like self-driving cars, facial recognition, and object detection.\n\n**Applications:**\n\n1. **Virtual Assistants:** AI-powered virtual assistants like Siri, Alexa, and Google Assistant have become increasingly popular, making it easier for people to interact with technology.\n2. **Healthcare:** AI is being used in healthcare to analyze medical images, diagnose diseases, and develop personalized treatment plans.\n3. **Business:** AI is being used in business to automate tasks, predict customer behavior, and optimize supply chains.\n\n**Challenges and Concerns:**\n\n1. **Job Displacement:** The increasing use of AI has raised concerns about job displacement, as automation replaces certain jobs.\n2. **Bias and Ethics:** AI systems can perpetuate biases and raise ethical concerns, such as privacy and accountability.\n3. **Security:** AI systems can be vulnerable to cyber attacks, highlighting the need for robust security measures.\n\n**Conclusion:**\nAI has the potential to revolutionize various aspects of our lives, from healthcare and education to business and entertainment. However, it is essential to address the challenges and concerns associated with AI development and deployment, ensuring that its benefits are realized while minimizing its risks. As AI continues to evolve, it is crucial to prioritize responsible AI development, deployment, and regulation."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_fact(x):\n",
        "    if \"\\n\\n\" in x:\n",
        "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "old_word = \"AI\"\n",
        "new_word = \"skynet\"\n",
        "\n",
        "def replace_word(x):\n",
        "    return x.replace(old_word, new_word)"
      ],
      "metadata": {
        "id": "A7SYlXbGVTDh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_fact_runnable = RunnableLambda(extract_fact)\n",
        "replace_word_runnable = RunnableLambda(replace_word)"
      ],
      "metadata": {
        "id": "Fx2JKEqFVopn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
      ],
      "metadata": {
        "id": "j8h8h_8yV7WN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke(\"AI\")\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "EYKy4E9PV90q",
        "outputId": "7aa1054f-fa75-4c4d-fab0-f06d22eb8f33"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Introduction:**\nArtificial Intelligence (skynet) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. skynet has been rapidly advancing in recent years, with significant breakthroughs in areas like machine learning, natural language processing, and computer vision.\n**Key Developments:**\n1. **Machine Learning:** skynet algorithms can now learn from large datasets, enabling applications like image recognition, speech recognition, and predictive analytics.\n2. **Natural Language Processing (NLP):** skynet-powered NLP has improved significantly, allowing for more accurate language translation, sentiment analysis, and text summarization.\n3. **Computer Vision:** skynet-powered computer vision has enabled applications like self-driving cars, facial recognition, and object detection.\n**Applications:**\n1. **Virtual Assistants:** skynet-powered virtual assistants like Siri, Alexa, and Google Assistant have become increasingly popular, making it easier for people to interact with technology.\n2. **Healthcare:** skynet is being used in healthcare to analyze medical images, diagnose diseases, and develop personalized treatment plans.\n3. **Finance:** skynet is being used in finance to detect fraud, predict stock prices, and optimize investment portfolios.\n**Challenges and Concerns:**\n1. **Job Displacement:** The increasing use of skynet has raised concerns about job displacement, as automation replaces certain jobs.\n2. **Bias and Ethics:** skynet systems can perpetuate biases and discriminate against certain groups, highlighting the need for more diverse and inclusive skynet development.\n3. **Security:** skynet systems can be vulnerable to cyber attacks, emphasizing the need for robust security measures.\n**Conclusion:**\nskynet has the potential to revolutionize numerous industries and aspects of our lives. While there are challenges and concerns associated with skynet, the benefits of skynet, such as improved efficiency, accuracy, and decision-making, make it an exciting and rapidly evolving field. As skynet continues to advance, it is essential to address the challenges and concerns associated with its development and deployment."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_f9d8UgEV_0K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}